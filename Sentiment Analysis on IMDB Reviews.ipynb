{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b810c24b4ec3721f1ba21f8533ccdb9fa77a544"
   },
   "source": [
    "# Sentiment Analysis on Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdb35c375feb9da290ef70440ed819ed6154be1e"
   },
   "source": [
    "In this notebook Sentiment Analysis is performed on movie reviews.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LSTM Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "ef3ea28d8941735e353645478b9844df25497840"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "from multiplicative_lstm import MultiplicativeLSTM\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet \n",
    "allEnglishWords = words.words() + [w for w in wordnet.words()]\n",
    "allEnglishWords = np.unique([x.lower() for x in allEnglishWords])\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from multiplicative_lstm import MultiplicativeLSTM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24e1f135256fa1e76453e3fbb302497f0486771f"
   },
   "source": [
    "---\n",
    "\n",
    "## Legacy Sentiment Analysis Data Import\n",
    "First, we need to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reviews(base_path, folder):\n",
    "    file_names = [x for x in os.listdir(base_path+folder) if x.endswith(\".txt\")]\n",
    "    reviews = []\n",
    "    for file_name in file_names:\n",
    "        ID, rating = file_name[:-4].split(\"_\") # Remove .txt and split filename\n",
    "        \n",
    "        if int(rating) > 6 or int(rating) < 4:\n",
    "            label = 1 if int(rating) > 6 else 0\n",
    "            with open(base_path+folder+file_name, encoding=\"latin1\") as f:\n",
    "                reviews.append({\n",
    "                    \"label\": label,\n",
    "                    \"review\": f.read(),\n",
    "                    \"file\": file_name\n",
    "                })\n",
    "            \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b05c12defc2790556abcd81b183750ca6badb14a"
   },
   "outputs": [],
   "source": [
    "base_path = \"./aclImdb/\"\n",
    "\n",
    "trainReviews = parse_reviews(base_path, \"train/pos/\")\n",
    "trainReviews += parse_reviews(base_path, \"train/neg/\")\n",
    "\n",
    "testReviews = parse_reviews(base_path, \"test/pos/\")\n",
    "testReviews += parse_reviews(base_path, \"test/neg/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d1b2a5e2df68adaa443eb82365b340c4bac8879b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1670_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Channel 4 is a channel that allows more naught...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6841_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>This is probably one of Brian De Palma's best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2584_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>The film starts out very slowly, with the life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6835_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>It was once suggested by Pauline Kael, never a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>660_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>9/10- 30 minutes of pure holiday terror. Okay,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file  label                                             review\n",
       "0   1670_8.txt      1  Channel 4 is a channel that allows more naught...\n",
       "1   6841_7.txt      1  This is probably one of Brian De Palma's best ...\n",
       "2   2584_7.txt      1  The film starts out very slowly, with the life...\n",
       "3  6835_10.txt      1  It was once suggested by Pauline Kael, never a...\n",
       "4    660_9.txt      1  9/10- 30 minutes of pure holiday terror. Okay,..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(trainReviews)\n",
    "test_df = pd.DataFrame(testReviews)\n",
    "\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b0a081b837717a864ae9ad1c30cf5aa01acb8c4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "This is probably one of Brian De Palma's best known movies but it isn't his best. Body Double, The Fury and Carrie are better movies but this movie is better than Blow Out and Obsession. De Palma is very influenced by Hitchcock and this movie is a take off on Psycho. Angie Dickinson is a bored housewife who is thinking of having an affair and after her psychiatrist, played by Michael Caine, turns down an offer, Dickinson meets a man in a art gallery and she winds up sleeping with him. After this point it's best you don't know what happens but there is a murder and Nancy Allen is a call girl who gets a look at the killer. Dennis Franz is the detective on the case who really doesn't trust Allen and she has to find the killer herself. It's a pretty good movie but isn't one of De Palma's best."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(train_df.review.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc2aa83cb2974a57d696e3481eb3ae62082be434"
   },
   "source": [
    "---\n",
    "\n",
    "## Data Preprocessing\n",
    "The next step is data preprocessing. The following class behaves like your typical SKLearn vectorizer.\n",
    "\n",
    "It can perform the following operations.\n",
    "* Discard non alpha-numeric characters\n",
    "* Set everything to lower case\n",
    "* Stems all words using PorterStemmer, and change the stems back to the most occurring existent word.\n",
    "* Discard non-Egnlish words (not by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "5fcdb0c46e96a0abdf3ef83e8c27c02625a58b0d"
   },
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    ''' Preprocess data for NLP tasks. '''\n",
    "\n",
    "    def __init__(self, alpha=True, lower=True, stemmer=True, english=False):\n",
    "        self.alpha = alpha\n",
    "        self.lower = lower\n",
    "        self.stemmer = stemmer\n",
    "        self.english = english\n",
    "        \n",
    "        self.uniqueWords = None\n",
    "        self.uniqueStems = None\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        texts = self._doAlways(texts)\n",
    "\n",
    "        allwords = pd.DataFrame({\"word\": np.concatenate(texts.apply(lambda x: x.split()).values)})\n",
    "        self.uniqueWords = allwords.groupby([\"word\"]).size().rename(\"count\").reset_index()\n",
    "        self.uniqueWords = self.uniqueWords[self.uniqueWords[\"count\"]>1]\n",
    "        if self.stemmer:\n",
    "            self.uniqueWords[\"stem\"] = self.uniqueWords.word.apply(lambda x: PorterStemmer().stem(x)).values\n",
    "            self.uniqueWords.sort_values([\"stem\", \"count\"], inplace=True, ascending=False)\n",
    "            self.uniqueStems = self.uniqueWords.groupby(\"stem\").first()\n",
    "        \n",
    "        #if self.english: self.words[\"english\"] = np.in1d(self.words[\"mode\"], allEnglishWords)\n",
    "        print(\"Fitted.\")\n",
    "            \n",
    "    def transform(self, texts):\n",
    "        texts = self._doAlways(texts)\n",
    "        if self.stemmer:\n",
    "            allwords = np.concatenate(texts.apply(lambda x: x.split()).values)\n",
    "            uniqueWords = pd.DataFrame(index=np.unique(allwords))\n",
    "            uniqueWords[\"stem\"] = pd.Series(uniqueWords.index).apply(lambda x: PorterStemmer().stem(x)).values\n",
    "            uniqueWords[\"mode\"] = uniqueWords.stem.apply(lambda x: self.uniqueStems.loc[x, \"word\"] if x in self.uniqueStems.index else \"\")\n",
    "            texts = texts.apply(lambda x: \" \".join([uniqueWords.loc[y, \"mode\"] for y in x.split()]))\n",
    "        #if self.english: texts = self.words.apply(lambda x: \" \".join([y for y in x.split() if self.words.loc[y,\"english\"]]))\n",
    "        print(\"Transformed.\")\n",
    "        return(texts)\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        texts = self._doAlways(texts)\n",
    "        self.fit(texts)\n",
    "        texts = self.transform(texts)\n",
    "        return(texts)\n",
    "    \n",
    "    def _doAlways(self, texts):\n",
    "        # Remove parts between <>'s\n",
    "        texts = texts.apply(lambda x: re.sub('<.*?>', ' ', x))\n",
    "        # Keep letters and digits only.\n",
    "        if self.alpha: texts = texts.apply(lambda x: re.sub('[^a-zA-Z0-9 ]+', ' ', x))\n",
    "        # Set everything to lower case\n",
    "        if self.lower: texts = texts.apply(lambda x: x.lower())\n",
    "        return texts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "df22fbf78cbc9330c7d4d842544a41111822aefa"
   },
   "outputs": [],
   "source": [
    "preprocess = Preprocessor(alpha=True, lower=True, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "079f43c870081d6596bb1acae19c7346e9a31574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted.\n",
      "Transformed.\n",
      "Transformed.\n",
      "CPU times: user 1min 56s, sys: 2.37 s, total: 1min 58s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainX = preprocess.fit_transform(train_df.review).values\n",
    "testX = preprocess.transform(test_df.review).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is probably one of brian de palma s best known movie but it isn t his best body double the fury and carry are better movie but this movie is better than blow out and obsessed de palma is very influence by hitchcock and this movie is a take off on psycho angie dickinson is a boring housewife who is think of have an affair and after her psychiatrist played by michael caine turn down an offer dickinson meets a man in a art gallery and she wind up sleep with him after this point it s best you don t know what happened but there is a murder and nancy allen is a called girl who get a look at the killer dennis franz is the detective on the case who really doesn t trust allen and she has to find the killer herself it s a pretty good movie but isn t one of de palma s best\n"
     ]
    }
   ],
   "source": [
    "print(trainX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "c9768ca9cae490e60bd3d35398645eb6c8fd228f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44448, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17562</th>\n",
       "      <td>disappointingly</td>\n",
       "      <td>12</td>\n",
       "      <td>disappointingli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17560</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>770</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17561</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>327</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17563</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>323</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17559</th>\n",
       "      <td>disappoint</td>\n",
       "      <td>89</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17566</th>\n",
       "      <td>disappoints</td>\n",
       "      <td>27</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17565</th>\n",
       "      <td>disappointments</td>\n",
       "      <td>20</td>\n",
       "      <td>disappoint</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  count             stem\n",
       "17562  disappointingly     12  disappointingli\n",
       "17560     disappointed    770       disappoint\n",
       "17561    disappointing    327       disappoint\n",
       "17563   disappointment    323       disappoint\n",
       "17559       disappoint     89       disappoint\n",
       "17566      disappoints     27       disappoint\n",
       "17565  disappointments     20       disappoint"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preprocess.uniqueWords.shape)\n",
    "preprocess.uniqueWords[preprocess.uniqueWords.word.str.contains(\"disappoint\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "38a34b398e3199006c90d9f8318c14f6cc1cc370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29254, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stem</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>disappoint</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointingli</th>\n",
       "      <td>disappointingly</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            word  count\n",
       "stem                                   \n",
       "disappoint          disappointed    770\n",
       "disappointingli  disappointingly     12"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preprocess.uniqueStems.shape)\n",
    "preprocess.uniqueStems[preprocess.uniqueStems.word.str.contains(\"disappoint\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44b462bbff3ae1fc92cac57b9f4ded43019018c8"
   },
   "source": [
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "Next, we take the preprocessed texts as input and calculate their TF-IDF's ([info](http://www.tfidf.com)). We retain 10000 features per text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "87ea6e08cfabc83b677977c6cbf2a9ad1facacd6"
   },
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union([\"thats\",\"weve\",\"dont\",\"lets\",\"youre\",\"im\",\"thi\",\"ha\",\n",
    "    \"wa\",\"st\",\"ask\",\"want\",\"like\",\"thank\",\"know\",\"susan\",\"ryan\",\"say\",\"got\",\"ought\",\"ive\",\"theyre\"])\n",
    "tfidf = TfidfVectorizer(min_df=2, max_features=10000, stop_words=stop_words) #, ngram_range=(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "26bb2efc663fbfa4980b97421b6fcf5d8aa95aac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.6 s, sys: 1.77 s, total: 8.37 s\n",
      "Wall time: 8.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainX_tfidf = tfidf.fit_transform(trainX).toarray()\n",
    "testX_tfidf = tfidf.transform(testX).toarray()\n",
    "\n",
    "trainY = train_df.label\n",
    "testY = test_df.label\n",
    "\n",
    "display(trainY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(trainX_tfidf[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66bb65fc219ab8c5e8bc0b9eca79ea2fa77911f4"
   },
   "source": [
    "---\n",
    "\n",
    "## Feature Selection\n",
    "Next, we take the 10k dimensional tfidf's as input, and keep the 2000 dimensions that correlate the most with our sentiment target. The corresponding words - see below - make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "d921ce51e8012db3d74511741c3c61fe1ca9f08a"
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "efe0c4d4d870f4d6d5d9b367111e45d6503a389d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01780022 -0.01936224  0.00697161 ...  0.01920079  0.00850365\n",
      " -0.00796341]\n"
     ]
    }
   ],
   "source": [
    "getCorrelation = np.vectorize(lambda x: pearsonr(trainX_tfidf[:,x], trainY)[0])\n",
    "correlations = getCorrelation(np.arange(trainX_tfidf.shape[1]))\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "bc89e481eb151a16964b4cd725fad86a3446dfb4"
   },
   "outputs": [],
   "source": [
    "allIndices = np.argsort(-correlations)\n",
    "bestIndices = allIndices[np.concatenate([np.arange(1000), np.arange(-1000, 0)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "9bedebdc1406e24d21ef775af93ce16d01d1ff5e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great' 'love' 'excellent' 'best' 'beautiful' 'perfect' 'performance'\n",
      " 'favorite' 'enjoy' 'amazing']\n",
      "['money' 'stupid' 'horrible' 'worse' 'boring' 'terrible' 'awful' 'waste'\n",
      " 'worst' 'bad']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = np.array(tfidf.get_feature_names())\n",
    "print(vocabulary[bestIndices][:10])\n",
    "print(vocabulary[bestIndices][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "5ec3ab480f29519cf862400e472d494ef59c6afe"
   },
   "outputs": [],
   "source": [
    "trainX_engr = trainX_tfidf[:,bestIndices]\n",
    "testX_engr = testX_tfidf[:,bestIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "ea076abbc8e375b7f5887e545c2a93e99a74039c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(22304, 2000) (22365, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(trainX_engr[0].shape)\n",
    "print(trainX_engr.shape, testX_engr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.90\n",
      "Best parameters:  {'C': 10}\n",
      "Best estimator:  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 20]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(trainX_engr, trainY)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reg = LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "model_reg.fit(trainX_engr, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8977867203219316\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cross_val_score(model_reg, testX_engr, testY, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "851531479bd0741b7b85f0d66feb518f5e8f6884"
   },
   "source": [
    "---\n",
    "\n",
    "## Model Architecture\n",
    "We choose a very simple dense network with 6 layers, performing binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "4484d9f286282862d7f63fe743590d427ef8f87e"
   },
   "outputs": [],
   "source": [
    "DROPOUT = 0.5\n",
    "ACTIVATION = \"tanh\"\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential([    \n",
    "    Dense(int(trainX_engr.shape[1]/2), activation=ACTIVATION, input_dim=trainX_engr.shape[1]),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(int(trainX_engr.shape[1]/2), activation=ACTIVATION, input_dim=trainX_engr.shape[1]),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(int(trainX_engr.shape[1]/4), activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(100, activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(20, activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(5, activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(2000, 128))\n",
    "# model.add(MultiplicativeLSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "683cda4b76b1f12887e944f877b69d52d428edbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 105       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 3,554,731\n",
      "Trainable params: 3,554,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.compile(optimizer=optimizers.Adam(0.00005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a9bbc915144f49016162c7b48d1f6d0fab1fc0d"
   },
   "source": [
    "---\n",
    "\n",
    "## Model Training\n",
    "Let's go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "2dd8f0aa6829bca51b23c6d90192617d77bd4cd9"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCHSIZE = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "6276fd33bf3571a528bcc5d8cc816264b70e021e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14943 samples, validate on 7361 samples\n",
      "Epoch 1/30\n",
      "14943/14943 [==============================] - 2s 144us/step - loss: 0.5071 - acc: 0.7804 - val_loss: 0.8750 - val_acc: 0.4984\n",
      "Epoch 2/30\n",
      "14943/14943 [==============================] - 1s 61us/step - loss: 0.3140 - acc: 0.8947 - val_loss: 0.6741 - val_acc: 0.7464\n",
      "Epoch 3/30\n",
      "14943/14943 [==============================] - 1s 61us/step - loss: 0.2605 - acc: 0.9257 - val_loss: 0.5221 - val_acc: 0.8171\n",
      "Epoch 4/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.2350 - acc: 0.9337 - val_loss: 0.6525 - val_acc: 0.7765\n",
      "Epoch 5/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.2142 - acc: 0.9433 - val_loss: 0.8651 - val_acc: 0.7153\n",
      "Epoch 6/30\n",
      "14943/14943 [==============================] - 1s 59us/step - loss: 0.1979 - acc: 0.9488 - val_loss: 0.7990 - val_acc: 0.7443\n",
      "Epoch 7/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1895 - acc: 0.9523 - val_loss: 0.7648 - val_acc: 0.7598\n",
      "Epoch 8/30\n",
      "14943/14943 [==============================] - 1s 59us/step - loss: 0.1793 - acc: 0.9574 - val_loss: 0.9277 - val_acc: 0.7253\n",
      "Epoch 9/30\n",
      "14943/14943 [==============================] - 1s 59us/step - loss: 0.1820 - acc: 0.9573 - val_loss: 0.7734 - val_acc: 0.7704\n",
      "Epoch 10/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1770 - acc: 0.9552 - val_loss: 0.8206 - val_acc: 0.7631\n",
      "Epoch 11/30\n",
      "14943/14943 [==============================] - 1s 59us/step - loss: 0.1742 - acc: 0.9578 - val_loss: 0.9618 - val_acc: 0.7282\n",
      "Epoch 12/30\n",
      "14943/14943 [==============================] - 1s 59us/step - loss: 0.1727 - acc: 0.9610 - val_loss: 0.8386 - val_acc: 0.7593\n",
      "Epoch 13/30\n",
      "14943/14943 [==============================] - 1s 59us/step - loss: 0.1712 - acc: 0.9607 - val_loss: 0.8500 - val_acc: 0.7624\n",
      "Epoch 14/30\n",
      "14943/14943 [==============================] - 1s 58us/step - loss: 0.1602 - acc: 0.9619 - val_loss: 0.8598 - val_acc: 0.7646\n",
      "Epoch 15/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1570 - acc: 0.9637 - val_loss: 0.9768 - val_acc: 0.7381\n",
      "Epoch 16/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1582 - acc: 0.9646 - val_loss: 0.8821 - val_acc: 0.7680\n",
      "Epoch 17/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1498 - acc: 0.9660 - val_loss: 1.0198 - val_acc: 0.7348\n",
      "Epoch 18/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1572 - acc: 0.9651 - val_loss: 0.8921 - val_acc: 0.7677\n",
      "Epoch 19/30\n",
      "14943/14943 [==============================] - 1s 59us/step - loss: 0.1518 - acc: 0.9666 - val_loss: 1.0443 - val_acc: 0.7275\n",
      "Epoch 20/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1549 - acc: 0.9665 - val_loss: 1.0229 - val_acc: 0.7354\n",
      "Epoch 21/30\n",
      "14943/14943 [==============================] - 1s 61us/step - loss: 0.1458 - acc: 0.9676 - val_loss: 1.0535 - val_acc: 0.7331\n",
      "Epoch 22/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1418 - acc: 0.9703 - val_loss: 1.0392 - val_acc: 0.7404\n",
      "Epoch 23/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1423 - acc: 0.9698 - val_loss: 1.0190 - val_acc: 0.7494\n",
      "Epoch 24/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1415 - acc: 0.9696 - val_loss: 1.0025 - val_acc: 0.7552\n",
      "Epoch 25/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1489 - acc: 0.9656 - val_loss: 0.8814 - val_acc: 0.7855\n",
      "Epoch 26/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1525 - acc: 0.9662 - val_loss: 0.9908 - val_acc: 0.7544\n",
      "Epoch 27/30\n",
      "14943/14943 [==============================] - 1s 59us/step - loss: 0.1513 - acc: 0.9665 - val_loss: 1.0635 - val_acc: 0.7321\n",
      "Epoch 28/30\n",
      "14943/14943 [==============================] - 1s 58us/step - loss: 0.1447 - acc: 0.9689 - val_loss: 1.0308 - val_acc: 0.7441\n",
      "Epoch 29/30\n",
      "14943/14943 [==============================] - 1s 60us/step - loss: 0.1412 - acc: 0.9722 - val_loss: 1.1226 - val_acc: 0.7273\n",
      "Epoch 30/30\n",
      "14943/14943 [==============================] - 1s 59us/step - loss: 0.1368 - acc: 0.9722 - val_loss: 1.0512 - val_acc: 0.7481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f3f49d27828>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX_engr, trainY, epochs=EPOCHS, batch_size=BATCHSIZE, validation_split=0.33)\n",
    "\n",
    "# model.fit(trainX, trainY,\n",
    "#           batch_size=1500,\n",
    "#           epochs=15,\n",
    "#           validation_split=0.33,\n",
    "#           verbose=1,\n",
    "#           callbacks=[ModelCheckpoint('imdb_mlstm2.h5', monitor='val_acc',\n",
    "#                                      save_best_only=True, save_weights_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "65c8bb930bdd202430bb3a005f9d2d28b7bbd300"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "size": 5
         },
         "name": "Train Accuracy",
         "type": "scatter",
         "uid": "e74363ec-7171-4877-9329-3f6d2753b128",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "y": [
          0.7803653909532546,
          0.8947333191753127,
          0.9256508022133927,
          0.9336813118557833,
          0.9432510152755313,
          0.9488054623902502,
          0.9522853443670933,
          0.9574382626323474,
          0.9573044207435467,
          0.9552298748476442,
          0.9578397936357644,
          0.9609850774781098,
          0.9606504712303918,
          0.9618550474637485,
          0.9636619191671371,
          0.9645988106057481,
          0.9660041421394563,
          0.9650672537283447,
          0.966606434653787,
          0.9665395123033346,
          0.9676102479402618,
          0.9702870903472732,
          0.9698186423543513,
          0.96955095857675,
          0.965602624789703,
          0.9662049128285998,
          0.9665395245928287,
          0.9689486786750061,
          0.9721608815890099,
          0.9722278017017454
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "size": 5
         },
         "name": "Valid Accuracy",
         "type": "scatter",
         "uid": "8691631b-ddbf-4fd9-a96f-6554761165b7",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "y": [
          0.49843771194750935,
          0.7463659816195728,
          0.8171444005607311,
          0.7765249154470875,
          0.7152560709076979,
          0.744328221044306,
          0.7598152394901069,
          0.725309067430938,
          0.7704116239639716,
          0.7630756664917434,
          0.7281619307544953,
          0.7592718467134575,
          0.7623964166401553,
          0.76457002853314,
          0.7380790591984886,
          0.7679663139619738,
          0.7348186395330576,
          0.7676946082900293,
          0.7274826843604788,
          0.7353620421722877,
          0.7330525633579783,
          0.7403885233403872,
          0.7493547061234288,
          0.7551962919184054,
          0.7854911103599698,
          0.7543812005954857,
          0.7321016126442759,
          0.7440565130889067,
          0.7273468236012427,
          0.7481320520536259
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "size": 5
         },
         "name": "Train Loss",
         "type": "scatter",
         "uid": "02897fb9-2cb6-4d0a-8070-8f7feab60a77",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "y": [
          0.5071490745160552,
          0.31403259646100373,
          0.2604510000888374,
          0.2350433415478431,
          0.21420807945599066,
          0.19794525797827564,
          0.1894610839163149,
          0.17926938391741776,
          0.18203288900687067,
          0.177049570226856,
          0.17416534252289761,
          0.1727261967064027,
          0.17116757822883966,
          0.16017218491466761,
          0.15699250555749256,
          0.1582209604963005,
          0.14982012600227188,
          0.1571927894803564,
          0.15176996449896715,
          0.1549005840669983,
          0.14578994596442457,
          0.14184972458373013,
          0.14228857106543574,
          0.14153626281749387,
          0.14885508551329069,
          0.15250313451636677,
          0.15128327030191172,
          0.14466227848614538,
          0.1412374745965411,
          0.13676173163510977
         ]
        },
        {
         "marker": {
          "size": 5
         },
         "name": "Valid Loss",
         "type": "scatter",
         "uid": "9ed0cb44-5f54-4981-aec5-de7c93f2b99b",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29
         ],
         "y": [
          0.8749752571572753,
          0.6741374429194374,
          0.5220610432659798,
          0.6524727720716145,
          0.8651145493720248,
          0.7990386500790656,
          0.7647689868778399,
          0.9276813041125768,
          0.7734417820767109,
          0.8206250691054356,
          0.9617964514343817,
          0.8386138742537849,
          0.8499853777150197,
          0.8598498321797371,
          0.9768397077045562,
          0.8821143638567827,
          1.0197548783678931,
          0.8921223514641357,
          1.0442647724724257,
          1.022900465761995,
          1.053454975883625,
          1.039151098381289,
          1.0189540213726247,
          1.0025052528571798,
          0.8814037257830518,
          0.9907922518834716,
          1.063512177137622,
          1.0307518767202828,
          1.1225887527933525,
          1.051228222405193
         ]
        }
       ],
       "layout": {
        "font": {
         "family": "Palatino"
        },
        "title": {
         "text": "Model Training Evolution"
        },
        "xaxis": {
         "dtick": 1,
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Loss"
         }
        },
        "yaxis2": {
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Accuracy"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"a975126d-1231-44f8-b6d6-22a3d0894fc8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"a975126d-1231-44f8-b6d6-22a3d0894fc8\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'a975126d-1231-44f8-b6d6-22a3d0894fc8',\n",
       "                        [{\"marker\": {\"size\": 5}, \"name\": \"Train Accuracy\", \"type\": \"scatter\", \"uid\": \"25c2bcf0-9d7f-4860-ad72-7fc04284014c\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"y\": [0.7803653909532546, 0.8947333191753127, 0.9256508022133927, 0.9336813118557833, 0.9432510152755313, 0.9488054623902502, 0.9522853443670933, 0.9574382626323474, 0.9573044207435467, 0.9552298748476442, 0.9578397936357644, 0.9609850774781098, 0.9606504712303918, 0.9618550474637485, 0.9636619191671371, 0.9645988106057481, 0.9660041421394563, 0.9650672537283447, 0.966606434653787, 0.9665395123033346, 0.9676102479402618, 0.9702870903472732, 0.9698186423543513, 0.96955095857675, 0.965602624789703, 0.9662049128285998, 0.9665395245928287, 0.9689486786750061, 0.9721608815890099, 0.9722278017017454], \"yaxis\": \"y2\"}, {\"marker\": {\"size\": 5}, \"name\": \"Valid Accuracy\", \"type\": \"scatter\", \"uid\": \"860b6e3b-693f-4a67-a8d5-320bd72c7c4f\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"y\": [0.49843771194750935, 0.7463659816195728, 0.8171444005607311, 0.7765249154470875, 0.7152560709076979, 0.744328221044306, 0.7598152394901069, 0.725309067430938, 0.7704116239639716, 0.7630756664917434, 0.7281619307544953, 0.7592718467134575, 0.7623964166401553, 0.76457002853314, 0.7380790591984886, 0.7679663139619738, 0.7348186395330576, 0.7676946082900293, 0.7274826843604788, 0.7353620421722877, 0.7330525633579783, 0.7403885233403872, 0.7493547061234288, 0.7551962919184054, 0.7854911103599698, 0.7543812005954857, 0.7321016126442759, 0.7440565130889067, 0.7273468236012427, 0.7481320520536259], \"yaxis\": \"y2\"}, {\"marker\": {\"size\": 5}, \"name\": \"Train Loss\", \"type\": \"scatter\", \"uid\": \"8bf20b90-a6ae-4874-91ea-d94d6eb37583\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"y\": [0.5071490745160552, 0.31403259646100373, 0.2604510000888374, 0.2350433415478431, 0.21420807945599066, 0.19794525797827564, 0.1894610839163149, 0.17926938391741776, 0.18203288900687067, 0.177049570226856, 0.17416534252289761, 0.1727261967064027, 0.17116757822883966, 0.16017218491466761, 0.15699250555749256, 0.1582209604963005, 0.14982012600227188, 0.1571927894803564, 0.15176996449896715, 0.1549005840669983, 0.14578994596442457, 0.14184972458373013, 0.14228857106543574, 0.14153626281749387, 0.14885508551329069, 0.15250313451636677, 0.15128327030191172, 0.14466227848614538, 0.1412374745965411, 0.13676173163510977]}, {\"marker\": {\"size\": 5}, \"name\": \"Valid Loss\", \"type\": \"scatter\", \"uid\": \"9d555fe0-d651-4f6b-8bdd-d107b251e54b\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], \"y\": [0.8749752571572753, 0.6741374429194374, 0.5220610432659798, 0.6524727720716145, 0.8651145493720248, 0.7990386500790656, 0.7647689868778399, 0.9276813041125768, 0.7734417820767109, 0.8206250691054356, 0.9617964514343817, 0.8386138742537849, 0.8499853777150197, 0.8598498321797371, 0.9768397077045562, 0.8821143638567827, 1.0197548783678931, 0.8921223514641357, 1.0442647724724257, 1.022900465761995, 1.053454975883625, 1.039151098381289, 1.0189540213726247, 1.0025052528571798, 0.8814037257830518, 0.9907922518834716, 1.063512177137622, 1.0307518767202828, 1.1225887527933525, 1.051228222405193]}],\n",
       "                        {\"font\": {\"family\": \"Palatino\"}, \"title\": {\"text\": \"Model Training Evolution\"}, \"xaxis\": {\"dtick\": 1, \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"domain\": [0, 0.45], \"title\": {\"text\": \"Loss\"}}, \"yaxis2\": {\"domain\": [0.55, 1], \"title\": {\"text\": \"Accuracy\"}}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a975126d-1231-44f8-b6d6-22a3d0894fc8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(EPOCHS)\n",
    "history = model.history.history\n",
    "\n",
    "data = [\n",
    "    go.Scatter(x=x, y=history[\"acc\"], name=\"Train Accuracy\", marker=dict(size=5), yaxis='y2'),\n",
    "    go.Scatter(x=x, y=history[\"val_acc\"], name=\"Valid Accuracy\", marker=dict(size=5), yaxis='y2'),\n",
    "    go.Scatter(x=x, y=history[\"loss\"], name=\"Train Loss\", marker=dict(size=5)),\n",
    "    go.Scatter(x=x, y=history[\"val_loss\"], name=\"Valid Loss\", marker=dict(size=5))\n",
    "]\n",
    "layout = go.Layout(\n",
    "    title=\"Model Training Evolution\", font=dict(family='Palatino'), xaxis=dict(title='Epoch', dtick=1),\n",
    "    yaxis1=dict(title=\"Loss\", domain=[0, 0.45]), yaxis2=dict(title=\"Accuracy\", domain=[0.55, 1]),\n",
    ")\n",
    "py.iplot(go.Figure(data=data, layout=layout), show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c228a8d8681ba328cfa5359b5f943be713def86"
   },
   "source": [
    "---\n",
    "\n",
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a39927e07ab1483b90464c5413bd9f862ba5e4c"
   },
   "source": [
    "### Accuracy & Loss\n",
    "Let's first centralize the probabilities and predictions with the original train and validation dataframes. Then we can print out the respective accuracies and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_uuid": "01f2979207282c2e4c718f60894d97b6baeeccc9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1670_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Channel 4 is a channel that allows more naught...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6841_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>This is probably one of Brian De Palma's best ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2584_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>The film starts out very slowly, with the life...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6835_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>It was once suggested by Pauline Kael, never a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>660_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>9/10- 30 minutes of pure holiday terror. Okay,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1104_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>The film opens with Bill Coles (Melvyn Douglas...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9653_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>First love is a desperately difficult subject ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9314_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Only saw this show a few times, but will live ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6600_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm going to keep this review short and sweet....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5284_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Father and son communicate very little. IN fac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4861_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Joan Fontaine is \"A Damsel in Distress\" in thi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9704_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't believe it's been ten years since this...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5725_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>This is what I wrote to some friends earlier:&lt;...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3571_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>I've seen this movie twice with my teenagers w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3166_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>This show has come so far. At first EVERYONE i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6113_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>After having seen \"Marrying Mafia\", I'd nearly...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7943_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>i love this TV series so much. it contains ani...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1770_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>In the previews, \"The 40 Year-Old Virgin\" boas...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8489_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>I teach Japanese for an online high school and...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8654_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>it's amazing that so many people that i know h...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12299_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Filmed by MGM on the same sets as the English ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5130_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>...may seem like an overstatement, but it is n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10576_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>When it came out, this was pretty much state o...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6747_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>'Where the Sidewalk Ends (1950)' opens, approp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7113_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Being from the Philadelphia suburbs and extrem...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3129_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>This is one of the best reunion specials ever,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11758_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Spoiler This movie is about such a concept. Wi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3955_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>I cannot believe how unknown this movie is,it ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.245601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1792_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>I had the pleasure of viewing this beautiful f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2627_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Finally, Timon and Pumbaa in their own film......</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22274</th>\n",
       "      <td>6340_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This is the kind of movie i fear the most. Arr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22275</th>\n",
       "      <td>5685_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>The name Uwe Boll is automatically linked to b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22276</th>\n",
       "      <td>8906_3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This is another example of a sucky sequel to a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22277</th>\n",
       "      <td>6950_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>We all knew even before it aired, the Ron Moor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22278</th>\n",
       "      <td>7305_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>The premise of Cabin Fever starts like it MIGH...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22279</th>\n",
       "      <td>4883_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>During the cheap filmed in video beginning of ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22280</th>\n",
       "      <td>10870_3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Bill (Buddy Rogers) is sent to New York by his...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22281</th>\n",
       "      <td>6495_3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I rented this movie on the merits of what the ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22282</th>\n",
       "      <td>6428_3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This makes the third Errol Morris movie I've s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22283</th>\n",
       "      <td>11164_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>What a disappointment!&lt;br /&gt;&lt;br /&gt;This film se...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22284</th>\n",
       "      <td>2546_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie is an idiotic attempt at some kind ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22285</th>\n",
       "      <td>1771_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This has to be one of the worst films I have e...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22286</th>\n",
       "      <td>10128_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>he was my hero for all time until he went alon...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22287</th>\n",
       "      <td>186_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I didn't know whether to laugh or cry at this ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22288</th>\n",
       "      <td>6500_3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Look, I'm sorry if half the world takes offens...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22289</th>\n",
       "      <td>8076_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This su*k! Why do they have to make movies tha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22290</th>\n",
       "      <td>4571_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>What The Bleep Do We Know is a deluded and hap...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22291</th>\n",
       "      <td>11420_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>St. Elmo's Fire has no bearing on life after u...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.988818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22292</th>\n",
       "      <td>5884_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I am a big movie fan. I like movies of all typ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22293</th>\n",
       "      <td>8620_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I was unsure whether or not Andy Sidaris could...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22294</th>\n",
       "      <td>8677_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I liked Chiba in Street Fighter, and I figured...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22295</th>\n",
       "      <td>8170_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>A friend and I went through a phase some (alot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22296</th>\n",
       "      <td>12265_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>The special effects of this movie are, especia...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22297</th>\n",
       "      <td>9481_3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>James Bishop (Matt Stasi) goes to a `mental il...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22298</th>\n",
       "      <td>12305_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I am watching this movie right now on WTN beca...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22299</th>\n",
       "      <td>10101_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Notice I have given this 1 star if the option ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22300</th>\n",
       "      <td>10452_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I saw the 10p.m. showing and I must say that t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22301</th>\n",
       "      <td>3118_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I felt that the movie was dry... very disappoi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22302</th>\n",
       "      <td>5765_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Scary Movie 3 (2003) was a bad idea to begin w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22303</th>\n",
       "      <td>11406_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>But I got over it. To me, it seemed that even ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22304 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file  label                                             review  \\\n",
       "0       1670_8.txt      1  Channel 4 is a channel that allows more naught...   \n",
       "1       6841_7.txt      1  This is probably one of Brian De Palma's best ...   \n",
       "2       2584_7.txt      1  The film starts out very slowly, with the life...   \n",
       "3      6835_10.txt      1  It was once suggested by Pauline Kael, never a...   \n",
       "4        660_9.txt      1  9/10- 30 minutes of pure holiday terror. Okay,...   \n",
       "5       1104_8.txt      1  The film opens with Bill Coles (Melvyn Douglas...   \n",
       "6       9653_9.txt      1  First love is a desperately difficult subject ...   \n",
       "7      9314_10.txt      1  Only saw this show a few times, but will live ...   \n",
       "8       6600_9.txt      1  I'm going to keep this review short and sweet....   \n",
       "9       5284_9.txt      1  Father and son communicate very little. IN fac...   \n",
       "10      4861_8.txt      1  Joan Fontaine is \"A Damsel in Distress\" in thi...   \n",
       "11     9704_10.txt      1  I can't believe it's been ten years since this...   \n",
       "12      5725_9.txt      1  This is what I wrote to some friends earlier:<...   \n",
       "13      3571_7.txt      1  I've seen this movie twice with my teenagers w...   \n",
       "14      3166_9.txt      1  This show has come so far. At first EVERYONE i...   \n",
       "15      6113_8.txt      1  After having seen \"Marrying Mafia\", I'd nearly...   \n",
       "16     7943_10.txt      1  i love this TV series so much. it contains ani...   \n",
       "17     1770_10.txt      1  In the previews, \"The 40 Year-Old Virgin\" boas...   \n",
       "18      8489_9.txt      1  I teach Japanese for an online high school and...   \n",
       "19      8654_9.txt      1  it's amazing that so many people that i know h...   \n",
       "20     12299_7.txt      1  Filmed by MGM on the same sets as the English ...   \n",
       "21     5130_10.txt      1  ...may seem like an overstatement, but it is n...   \n",
       "22     10576_7.txt      1  When it came out, this was pretty much state o...   \n",
       "23      6747_8.txt      1  'Where the Sidewalk Ends (1950)' opens, approp...   \n",
       "24      7113_8.txt      1  Being from the Philadelphia suburbs and extrem...   \n",
       "25     3129_10.txt      1  This is one of the best reunion specials ever,...   \n",
       "26     11758_7.txt      1  Spoiler This movie is about such a concept. Wi...   \n",
       "27      3955_9.txt      1  I cannot believe how unknown this movie is,it ...   \n",
       "28     1792_10.txt      1  I had the pleasure of viewing this beautiful f...   \n",
       "29      2627_9.txt      1  Finally, Timon and Pumbaa in their own film......   \n",
       "...            ...    ...                                                ...   \n",
       "22274   6340_1.txt      0  This is the kind of movie i fear the most. Arr...   \n",
       "22275   5685_1.txt      0  The name Uwe Boll is automatically linked to b...   \n",
       "22276   8906_3.txt      0  This is another example of a sucky sequel to a...   \n",
       "22277   6950_1.txt      0  We all knew even before it aired, the Ron Moor...   \n",
       "22278   7305_2.txt      0  The premise of Cabin Fever starts like it MIGH...   \n",
       "22279   4883_1.txt      0  During the cheap filmed in video beginning of ...   \n",
       "22280  10870_3.txt      0  Bill (Buddy Rogers) is sent to New York by his...   \n",
       "22281   6495_3.txt      0  I rented this movie on the merits of what the ...   \n",
       "22282   6428_3.txt      0  This makes the third Errol Morris movie I've s...   \n",
       "22283  11164_1.txt      0  What a disappointment!<br /><br />This film se...   \n",
       "22284   2546_1.txt      0  This movie is an idiotic attempt at some kind ...   \n",
       "22285   1771_2.txt      0  This has to be one of the worst films I have e...   \n",
       "22286  10128_1.txt      0  he was my hero for all time until he went alon...   \n",
       "22287    186_1.txt      0  I didn't know whether to laugh or cry at this ...   \n",
       "22288   6500_3.txt      0  Look, I'm sorry if half the world takes offens...   \n",
       "22289   8076_1.txt      0  This su*k! Why do they have to make movies tha...   \n",
       "22290   4571_1.txt      0  What The Bleep Do We Know is a deluded and hap...   \n",
       "22291  11420_1.txt      0  St. Elmo's Fire has no bearing on life after u...   \n",
       "22292   5884_1.txt      0  I am a big movie fan. I like movies of all typ...   \n",
       "22293   8620_2.txt      0  I was unsure whether or not Andy Sidaris could...   \n",
       "22294   8677_2.txt      0  I liked Chiba in Street Fighter, and I figured...   \n",
       "22295   8170_2.txt      0  A friend and I went through a phase some (alot...   \n",
       "22296  12265_1.txt      0  The special effects of this movie are, especia...   \n",
       "22297   9481_3.txt      0  James Bishop (Matt Stasi) goes to a `mental il...   \n",
       "22298  12305_2.txt      0  I am watching this movie right now on WTN beca...   \n",
       "22299  10101_1.txt      0  Notice I have given this 1 star if the option ...   \n",
       "22300  10452_1.txt      0  I saw the 10p.m. showing and I must say that t...   \n",
       "22301   3118_1.txt      0  I felt that the movie was dry... very disappoi...   \n",
       "22302   5765_1.txt      0  Scary Movie 3 (2003) was a bad idea to begin w...   \n",
       "22303  11406_1.txt      0  But I got over it. To me, it seemed that even ...   \n",
       "\n",
       "       prediction  probability  \n",
       "0             1.0     0.990652  \n",
       "1             1.0     0.990638  \n",
       "2             1.0     0.990510  \n",
       "3             1.0     0.989994  \n",
       "4             1.0     0.990659  \n",
       "5             1.0     0.990726  \n",
       "6             1.0     0.990698  \n",
       "7             1.0     0.990722  \n",
       "8             1.0     0.990724  \n",
       "9             1.0     0.990721  \n",
       "10            1.0     0.990724  \n",
       "11            1.0     0.990720  \n",
       "12            1.0     0.990707  \n",
       "13            1.0     0.990721  \n",
       "14            1.0     0.990722  \n",
       "15            1.0     0.990724  \n",
       "16            1.0     0.990720  \n",
       "17            1.0     0.990670  \n",
       "18            1.0     0.990221  \n",
       "19            1.0     0.990715  \n",
       "20            1.0     0.990705  \n",
       "21            1.0     0.980226  \n",
       "22            1.0     0.990716  \n",
       "23            1.0     0.990689  \n",
       "24            1.0     0.990722  \n",
       "25            1.0     0.990718  \n",
       "26            1.0     0.989938  \n",
       "27            0.0     0.245601  \n",
       "28            1.0     0.990722  \n",
       "29            1.0     0.990688  \n",
       "...           ...          ...  \n",
       "22274         0.0     0.013146  \n",
       "22275         0.0     0.013148  \n",
       "22276         1.0     0.990379  \n",
       "22277         0.0     0.017382  \n",
       "22278         0.0     0.013242  \n",
       "22279         0.0     0.014494  \n",
       "22280         1.0     0.846670  \n",
       "22281         0.0     0.015675  \n",
       "22282         1.0     0.990655  \n",
       "22283         0.0     0.013145  \n",
       "22284         0.0     0.013301  \n",
       "22285         1.0     0.990618  \n",
       "22286         0.0     0.013170  \n",
       "22287         1.0     0.989952  \n",
       "22288         0.0     0.013178  \n",
       "22289         0.0     0.017920  \n",
       "22290         0.0     0.013151  \n",
       "22291         1.0     0.988818  \n",
       "22292         1.0     0.990639  \n",
       "22293         1.0     0.990714  \n",
       "22294         0.0     0.013149  \n",
       "22295         1.0     0.990497  \n",
       "22296         0.0     0.017022  \n",
       "22297         0.0     0.013164  \n",
       "22298         0.0     0.013457  \n",
       "22299         0.0     0.013222  \n",
       "22300         0.0     0.013216  \n",
       "22301         0.0     0.013157  \n",
       "22302         0.0     0.013158  \n",
       "22303         0.0     0.013391  \n",
       "\n",
       "[22304 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"prediction\"] = np.round(model.predict(trainX_engr))\n",
    "train_df[\"probability\"] = model.predict(trainX_engr)\n",
    "\n",
    "test_df[\"prediction\"] = np.round(model.predict(testX_engr))\n",
    "test_df[\"probability\"] = model.predict(testX_engr)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "f185efce8414c7f11badedc9c6a0ef62584a6785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22304/22304 [==============================] - 1s 57us/step\n",
      "[0.4016041257184859, 0.9045911047345767]\n",
      "0.9045911047345767\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(trainX_engr, trainY))\n",
    "print((train_df.label==train_df.prediction).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65734bdfebc04778bacfdcbe866ae881ffed33de"
   },
   "source": [
    "### Error Analysis\n",
    "Error analysis gives us great insight in the way the model is making its errors. Often, it shows data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "f9161fae002303d3947f31b10f0c8421641fb4f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>7782</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2022</td>\n",
       "      <td>12394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label          0      1\n",
       "prediction             \n",
       "0.0         7782    106\n",
       "1.0         2022  12394"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainCross = train_df.groupby([\"prediction\", \"label\"]).size().unstack()\n",
    "trainCross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "3d4454b148ac449b92296a6672cd331566f85233"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>7091</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2774</td>\n",
       "      <td>11747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label          0      1\n",
       "prediction             \n",
       "0.0         7091    753\n",
       "1.0         2774  11747"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validCross = test_df.groupby([\"prediction\", \"label\"]).size().unstack()\n",
    "validCross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_uuid": "a903053503fff95fb55527f82c8dcb1b55323652",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11747 true positives.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7357</th>\n",
       "      <td>6676_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>A beautiful, magical, thought-provoking and he...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9602</th>\n",
       "      <td>4453_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Cult-director Lucio Fulci is probably most fam...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900</th>\n",
       "      <td>234_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Although I'm not crazy about musicals, COVER G...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file  label                                             review  \\\n",
       "7357  6676_10.txt      1  A beautiful, magical, thought-provoking and he...   \n",
       "9602  4453_10.txt      1  Cult-director Lucio Fulci is probably most fam...   \n",
       "4900    234_7.txt      1  Although I'm not crazy about musicals, COVER G...   \n",
       "\n",
       "      prediction  probability  \n",
       "7357         1.0     0.990728  \n",
       "9602         1.0     0.990728  \n",
       "4900         1.0     0.990728  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truepositives = test_df[(test_df.label==True)&(test_df.label==test_df.prediction)]\n",
    "print(len(truepositives), \"true positives.\")\n",
    "truepositives.sort_values(\"probability\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "_uuid": "493772512c1c7fbf62b11960ae8bc5c4e29a2c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7091 true negatives.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21115</th>\n",
       "      <td>5183_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie is just downright horrible, the mov...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16708</th>\n",
       "      <td>4040_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This is just the same old crap that is spewed ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17225</th>\n",
       "      <td>7820_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>For years I hesitated watching this movie. Now...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file  label                                             review  \\\n",
       "21115  5183_1.txt      0  This movie is just downright horrible, the mov...   \n",
       "16708  4040_2.txt      0  This is just the same old crap that is spewed ...   \n",
       "17225  7820_1.txt      0  For years I hesitated watching this movie. Now...   \n",
       "\n",
       "       prediction  probability  \n",
       "21115         0.0     0.013142  \n",
       "16708         0.0     0.013142  \n",
       "17225         0.0     0.013142  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truenegatives = test_df[(test_df.label==False)&(test_df.label==test_df.prediction)]\n",
    "print(len(truenegatives), \"true negatives.\")\n",
    "truenegatives.sort_values(\"probability\", ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_uuid": "d58e3a6900933a499276c6431b18cf56fc57ad77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753 false positives.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>5744_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Right, here we go, you have probably read in p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>8565_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>It's not Citizen Kane, but it does deliver. Cl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959</th>\n",
       "      <td>6275_9.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Miles O'Keeffe once again assumes the role of ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file  label                                             review  \\\n",
       "4633  5744_10.txt      1  Right, here we go, you have probably read in p...   \n",
       "4801   8565_9.txt      1  It's not Citizen Kane, but it does deliver. Cl...   \n",
       "5959   6275_9.txt      1  Miles O'Keeffe once again assumes the role of ...   \n",
       "\n",
       "      prediction  probability  \n",
       "4633         0.0     0.013145  \n",
       "4801         0.0     0.013145  \n",
       "5959         0.0     0.013153  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falsepositives = test_df[(test_df.label==True)&(test_df.label!=test_df.prediction)]\n",
    "print(len(falsepositives), \"false positives.\")\n",
    "falsepositives.sort_values(\"probability\", ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "_uuid": "6146fa59be349d7443db78398f4ef222bf246feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2774 false negatives.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13939</th>\n",
       "      <td>6221_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I love musicals, all of them, from joyous Okla...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21807</th>\n",
       "      <td>12130_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie was pure genius. John Waters is bri...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20513</th>\n",
       "      <td>12253_3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Well, I must say, I initially found this short...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              file  label                                             review  \\\n",
       "13939   6221_1.txt      0  I love musicals, all of them, from joyous Okla...   \n",
       "21807  12130_1.txt      0  This movie was pure genius. John Waters is bri...   \n",
       "20513  12253_3.txt      0  Well, I must say, I initially found this short...   \n",
       "\n",
       "       prediction  probability  \n",
       "13939         1.0     0.990726  \n",
       "21807         1.0     0.990726  \n",
       "20513         1.0     0.990725  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falsenegatives = test_df[(test_df.label==False)&(test_df.label!=test_df.prediction)]\n",
    "print(len(falsenegatives), \"false negatives.\")\n",
    "falsenegatives.sort_values(\"probability\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "090480548ce39851c39c514f28482fd9c2e4a0f8"
   },
   "source": [
    "This is the review that got predicted as positive most certainly - while being labeled as negative. However, we can easily recognize it as a poorly labeled sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "521b966abe3ad3031a1824bc7b713d7f65eb708f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "I saw this movie when i was much younger and i thought it was funny. I saw it again last week, and you can guess the result. Some funny parts in it, very few and too long. The beginning is the only thing that is funny if you ask me.<br /><br />If you want a total b-movie this is a good pick, but don't expect too much from aliens dwarf size"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(valid.loc[22148].review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa2a5414267cb6b69ef3f73c44611641df51d71f"
   },
   "source": [
    "---\n",
    "\n",
    "## Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e908ad396fdc1bb655c2cff462fffdb31f73245e"
   },
   "source": [
    "### Custom Reviews\n",
    "To use this model, we would store the model, along with the preprocessing vectorizers, and run the unseen texts through following pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "_uuid": "707e470e97da1bb0a9b7b6692c5bfbc2cec31f44"
   },
   "outputs": [],
   "source": [
    "unseen = pd.Series(\"this movie is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_uuid": "84fc80a824af3ea9971c05fbb398386df8f2d3f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed.\n"
     ]
    }
   ],
   "source": [
    "unseen = preprocess.transform(unseen)       # Text preprocessing\n",
    "unseen = tfidf.transform(unseen).toarray()  # Feature engineering\n",
    "unseen = unseen[:,bestIndices]              # Feature selection\n",
    "probability = model.predict(unseen)[0,0]  # Network feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "_uuid": "7b02ab42a16ad85ba33a2fc4793e09adaed7d2ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9903882\n",
      "Positive!\n"
     ]
    }
   ],
   "source": [
    "print(probability)\n",
    "print(\"Positive!\") if probability > 0.5 else print(\"Negative!\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Content",
   "title_sidebar": "Content",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
